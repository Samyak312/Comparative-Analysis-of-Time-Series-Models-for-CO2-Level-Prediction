{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Load the Mauna Loa CO2 dataset\n",
    "data = pd.read_csv(\"co2_mm_mlo.csv\")\n",
    "\n",
    "# Preprocess the dataset for monthly readings\n",
    "monthly_data = data[['average']]\n",
    "monthly_data.index = pd.to_datetime(data[['year', 'month']].assign(day=1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(len(monthly_data) * 0.8)\n",
    "train_data, test_data = monthly_data[:train_size], monthly_data[train_size+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for MA, ARMA, and MLP models\n",
    "def moving_average(train, test, k):\n",
    "    predictions = []\n",
    "    history = list(train[-k:])\n",
    "    for t in range(len(test)):\n",
    "        yhat = np.mean(history)\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    return predictions\n",
    "\n",
    "def arma(train, test, p, q):\n",
    "    predictions = []\n",
    "    history = list(train)\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=(p, 0, q))\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    return predictions\n",
    "\n",
    "def mlp(train, test, k):\n",
    "    X_train, y_train = [], []\n",
    "    for i in range(len(train) - k):\n",
    "        X_train.append(train[i:i+k])\n",
    "        y_train.append(train[i+k])\n",
    "    X_test = [test[i:i+k] for i in range(len(test) - k)]\n",
    "    \n",
    "    mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000)\n",
    "    mlp_regressor.fit(X_train, y_train)\n",
    "    predictions = mlp_regressor.predict(X_test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for MLP: {'K': 3, 'T': 3}\n",
      "RMSE for Multi-Layer Perceptron (MLP): 1.6480238231604816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def find_best_mlp_params(train_values, k_folds=5):\n",
    "    param_grid = {\n",
    "        'K': [3, 6, 9, 12],\n",
    "        'T': [1, 2, 3]\n",
    "    }\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    kf = KFold(n_splits=k_folds)\n",
    "\n",
    "    for k_param in param_grid['K']:\n",
    "        for t_param in param_grid['T']:\n",
    "            rmse_sum = 0\n",
    "            for train_index, val_index in kf.split(train_values):\n",
    "                X_train_fold, X_val_fold = train_values[train_index], train_values[val_index]\n",
    "                mlp_forecasts = mlp(X_train_fold, X_val_fold, k_param)\n",
    "                rmse_fold = np.sqrt(mean_squared_error(X_val_fold[:len(mlp_forecasts)], mlp_forecasts))\n",
    "                rmse_sum += rmse_fold\n",
    "            avg_rmse = rmse_sum / k_folds\n",
    "            if avg_rmse < best_rmse:\n",
    "                best_rmse = avg_rmse\n",
    "                best_params['K'] = k_param\n",
    "                best_params['T'] = t_param\n",
    "\n",
    "    return best_params\n",
    "\n",
    "train_values = train_data['average'].values\n",
    "test_values = test_data['average'].values\n",
    "\n",
    "# Find the best parameters for MLP using k-fold cross-validation\n",
    "best_params = find_best_mlp_params(train_values)\n",
    "print(\"Best parameters for MLP:\", best_params)\n",
    "\n",
    "# Generate forecasts using the best parameters\n",
    "mlp_forecasts = mlp(train_values, test_values, best_params['K'])\n",
    "\n",
    "# Evaluate forecasts using RMSE\n",
    "mlp_rmse = np.sqrt(mean_squared_error(test_values[:len(mlp_forecasts)], mlp_forecasts))\n",
    "\n",
    "print(\"RMSE for Multi-Layer Perceptron (MLP):\", mlp_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "K = best_params['K']  \n",
    "T = best_params['T']  \n",
    "\n",
    "# Prepare data for modeling\n",
    "train_values = train_data['average'].values\n",
    "test_values = test_data['average'].values\n",
    "\n",
    "# Generate forecasts\n",
    "ma_forecasts = moving_average(train_values, test_values, K)\n",
    "arma_forecasts = arma(train_values, test_values, 1, 1)  # ARMA(1,1)\n",
    "mlp_forecasts = mlp(train_values, test_values, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Moving Average (MA): 9.934895856546449\n",
      "RMSE for AutoRegressive Moving Average (ARMA): 1.0479758441319134\n",
      "RMSE for Multi-Layer Perceptron (MLP): 1.2532738696720505\n"
     ]
    }
   ],
   "source": [
    "# Evaluate forecasts using RMSE\n",
    "ma_rmse = np.sqrt(mean_squared_error(test_values[:len(ma_forecasts)], ma_forecasts))\n",
    "arma_rmse = np.sqrt(mean_squared_error(test_values[:len(arma_forecasts)], arma_forecasts))\n",
    "mlp_rmse = np.sqrt(mean_squared_error(test_values[:len(mlp_forecasts)], mlp_forecasts))\n",
    "\n",
    "print(\"RMSE for Moving Average (MA):\", ma_rmse)\n",
    "print(\"RMSE for AutoRegressive Moving Average (ARMA):\", arma_rmse)\n",
    "print(\"RMSE for Multi-Layer Perceptron (MLP):\", mlp_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Average (MA) Model: \n",
    "    - The MA model typically has a high RMSE compared to more sophisticated models like ARMA and MLP. This is because the MA model only considers the historical average and does not account for any underlying trends or seasonality in the data.\n",
    "\n",
    "#### AutoRegressive Moving Average (ARMA) Model:\n",
    "    - The ARMA model shows the lowest RMSE among the three models, indicating the best forecasting performance in this particular analysis. Despite the warning messages regarding parameter initialization, the ARMA model demonstrates effectiveness in capturing the underlying patterns in the data.\n",
    "\n",
    "#### Multi-Layer Perceptron (MLP) Model: \n",
    "    - The MLP model achieves a higher RMSE compared to the ARMA model, suggesting slightly lower forecasting accuracy. While the MLP model typically has the capacity to capture complex patterns, in this instance, it appears that the ARMA model outperforms it.\n",
    "\n",
    "Therefore, in this specific analysis, the AutoRegressive Moving Average (ARMA) model appears to be the most effective for forecasting CO2 levels using the Mauna Loa dataset, followed by the Multi-Layer Perceptron (MLP) model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
